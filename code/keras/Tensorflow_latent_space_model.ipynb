{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/edward/stats/distributions.py:22: DeprecationWarning: edward.stats is deprecated. If calling rvs() from the distribution, use scipy.stats; if calling density methods from the distribution, use edward.models.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import edward as ed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.models import Normal\n",
    "from edward.stats import lognorm, norm, poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LatentSpaceModel():\n",
    "    \"\"\"\n",
    "    p(x, z) = [ prod_{i=1}^N prod{j=1}^N Poi(Y_{ij}; 1/ ||z_i - z_j||) ]\n",
    "              [ prod_{i=1}^N N(z_i; 0, I) ]\n",
    "    \"\"\"\n",
    "    def __init__(self, N, K, prior_std=1.0,\n",
    "                like=\"Poisson\",\n",
    "                prior=\"Lognormal\",\n",
    "                dist=\"euclidean\"):\n",
    "        self.n_vars = N * K\n",
    "        self.N = N\n",
    "        self.K = K\n",
    "        self.prior_std = prior_std\n",
    "        self.like = like\n",
    "        self.prior = prior\n",
    "        self.dist = dist\n",
    "        \n",
    "    def log_prob(self, xs, zs):\n",
    "        \"\"\"\n",
    "        Return Scaler the log joint density log p (xs, zs)\n",
    "        \"\"\"\n",
    "        if self.prior == \"Lognormal\":\n",
    "            log_prior = tf.reduce_sum(lognorm.logpdf(zs[\"z\"], self.prior_std))\n",
    "        elif self.prior == \"Gaussian\":\n",
    "            log_prior = tf.reduce_sum(norm.logpdf(zs[\"z\"], 0.0, self.prior_std))\n",
    "        else:\n",
    "            raise NotImplementedError(\"prior not available\")\n",
    "            \n",
    "        z = tf.reshape(zs[\"z\"], [self.N, self.K])\n",
    "        if self.dist == \"euclidean\":\n",
    "            xp = tf.tile(tf.reduce_sum(tf.pow(z, 2), 1, keep_dims=True), [1, self.N])\n",
    "            xp = xp + tf.transpose(xp) - 2 * tf.matmul(z, z, transpose_b=True)\n",
    "            xp = 1.0 / tf.sqrt(xp + tf.diag(tf.zeros(self.N) + 1e3))\n",
    "        elif self.dict == \"cosin\":\n",
    "            xp = tf.matmul(z, z, transpose_b=True)\n",
    "            \n",
    "        if self.like == \"Gaussian\":\n",
    "            log_lik = tf.reduce_sum(norm.logpdf(xs[\"x\"], xp, 1.0))\n",
    "        elif self.like == \"Poisson\":\n",
    "            if not (self.dist == \"euclidean\" or self.prior == \"Lognormal\"):\n",
    "                raise NotImplementedError(\"Rate of Poisson has to be nonnegatve\")\n",
    "            log_lik = tf.reduce_sum(poisson.logpmf(xs[\"x\"], xp))\n",
    "        else:\n",
    "            raise NotImplementedError(\"likelihood not available\")\n",
    "            \n",
    "        return log_lik + log_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ed.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/edward/inferences/inference.py:182: DeprecationWarning: Model wrappers are deprecated. Edward is dropping support for model wrappers in future versions; use the native language instead.\n",
      "  \"native language instead.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    1 [  0%]: Loss = 66886.383\n",
      "Iteration  250 [ 10%]: Loss = 38349.938\n",
      "Iteration  500 [ 20%]: Loss = 37068.918\n",
      "Iteration  750 [ 30%]: Loss = 36398.461\n",
      "Iteration 1000 [ 40%]: Loss = 36234.676\n",
      "Iteration 1250 [ 50%]: Loss = 35947.984\n",
      "Iteration 1500 [ 60%]: Loss = 35882.422\n",
      "Iteration 1750 [ 70%]: Loss = 35829.312\n",
      "Iteration 2000 [ 80%]: Loss = 35806.688\n",
      "Iteration 2250 [ 90%]: Loss = 35792.969\n",
      "Iteration 2500 [100%]: Loss = 35780.410\n"
     ]
    }
   ],
   "source": [
    "x_train = np.load(\"data/celegans_brain.npy\")\n",
    "\n",
    "model = LatentSpaceModel(N=x_train.shape[0], K=3, like=\"Poisson\", prior=\"Gaussian\")\n",
    "\n",
    "data = {\"x\": x_train}\n",
    "inference = ed.MAP([\"z\"], data, model)\n",
    "inference.run(n_iter=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_liks = x_post.log_prob(x_broadcasted)\n",
    "log_liks = tf.reduce_sum(log_liks, 3)\n",
    "log_liks = tf.reduce_mean(log_liks, 1)\n",
    "\n",
    "cluster = tf.argmax(log_liks, 1).eval()\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], c=cluster, cmap=cm.bwr)\n",
    "plt.axis([-3, 3, -3, 3])\n",
    "plt.title(\"Predicted cluster assignments\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
